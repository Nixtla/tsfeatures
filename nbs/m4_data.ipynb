{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# features\n",
    "\n",
    "> Fill in a module description here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |default_exp m4_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "seas_dict = {\n",
    "    \"Hourly\": {\"seasonality\": 24, \"input_size\": 24, \"output_size\": 48, \"freq\": \"H\"},\n",
    "    \"Daily\": {\"seasonality\": 7, \"input_size\": 7, \"output_size\": 14, \"freq\": \"D\"},\n",
    "    \"Weekly\": {\"seasonality\": 52, \"input_size\": 52, \"output_size\": 13, \"freq\": \"W\"},\n",
    "    \"Monthly\": {\"seasonality\": 12, \"input_size\": 12, \"output_size\": 18, \"freq\": \"M\"},\n",
    "    \"Quarterly\": {\"seasonality\": 4, \"input_size\": 4, \"output_size\": 8, \"freq\": \"Q\"},\n",
    "    \"Yearly\": {\"seasonality\": 1, \"input_size\": 4, \"output_size\": 6, \"freq\": \"D\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "SOURCE_URL = (\n",
    "    \"https://raw.githubusercontent.com/Mcompetitions/M4-methods/master/Dataset/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def maybe_download(filename, directory):\n",
    "    \"\"\"Download the data from M4's website, unless it's already here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename: str\n",
    "        Filename of M4 data with format /Type/Frequency.csv. Example: /Test/Daily-train.csv\n",
    "    directory: str\n",
    "        Custom directory where data will be downloaded.\n",
    "    \"\"\"\n",
    "    data_directory = directory + \"/m4\"\n",
    "    train_directory = data_directory + \"/Train/\"\n",
    "    test_directory = data_directory + \"/Test/\"\n",
    "\n",
    "    os.makedirs(data_directory, exist_ok=True)\n",
    "    os.makedirs(train_directory, exist_ok=True)\n",
    "    os.makedirs(test_directory, exist_ok=True)\n",
    "\n",
    "    filepath = os.path.join(data_directory, filename)\n",
    "    if not os.path.exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "\n",
    "        size = os.path.getsize(filepath)\n",
    "        print(\"Successfully downloaded\", filename, size, \"bytes.\")\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def m4_parser(dataset_name, directory, num_obs=1000000):\n",
    "    \"\"\"Transform M4 data into a panel.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: str\n",
    "        Frequency of the data. Example: 'Yearly'.\n",
    "    directory: str\n",
    "        Custom directory where data will be saved.\n",
    "    num_obs: int\n",
    "        Number of time series to return.\n",
    "    \"\"\"\n",
    "    data_directory = directory + \"/m4\"\n",
    "    train_directory = data_directory + \"/Train/\"\n",
    "    test_directory = data_directory + \"/Test/\"\n",
    "    freq = seas_dict[dataset_name][\"freq\"]\n",
    "\n",
    "    m4_info = pd.read_csv(data_directory + \"/M4-info.csv\", usecols=[\"M4id\", \"category\"])\n",
    "    m4_info = m4_info[m4_info[\"M4id\"].str.startswith(dataset_name[0])].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # Train data\n",
    "    train_path = \"{}{}-train.csv\".format(train_directory, dataset_name)\n",
    "\n",
    "    train_df = pd.read_csv(train_path, nrows=num_obs)\n",
    "    train_df = train_df.rename(columns={\"V1\": \"unique_id\"})\n",
    "\n",
    "    train_df = pd.wide_to_long(\n",
    "        train_df, stubnames=[\"V\"], i=\"unique_id\", j=\"ds\"\n",
    "    ).reset_index()\n",
    "    train_df = train_df.rename(columns={\"V\": \"y\"})\n",
    "    train_df = train_df.dropna()\n",
    "    train_df[\"split\"] = \"train\"\n",
    "    train_df[\"ds\"] = train_df[\"ds\"] - 1\n",
    "    # Get len of series per unique_id\n",
    "    len_series = train_df.groupby(\"unique_id\").agg({\"ds\": \"max\"}).reset_index()\n",
    "    len_series.columns = [\"unique_id\", \"len_serie\"]\n",
    "\n",
    "    # Test data\n",
    "    test_path = \"{}{}-test.csv\".format(test_directory, dataset_name)\n",
    "\n",
    "    test_df = pd.read_csv(test_path, nrows=num_obs)\n",
    "    test_df = test_df.rename(columns={\"V1\": \"unique_id\"})\n",
    "\n",
    "    test_df = pd.wide_to_long(\n",
    "        test_df, stubnames=[\"V\"], i=\"unique_id\", j=\"ds\"\n",
    "    ).reset_index()\n",
    "    test_df = test_df.rename(columns={\"V\": \"y\"})\n",
    "    test_df = test_df.dropna()\n",
    "    test_df[\"split\"] = \"test\"\n",
    "    test_df = test_df.merge(len_series, on=\"unique_id\")\n",
    "    test_df[\"ds\"] = test_df[\"ds\"] + test_df[\"len_serie\"] - 1\n",
    "    test_df = test_df[[\"unique_id\", \"ds\", \"y\", \"split\"]]\n",
    "\n",
    "    df = pd.concat((train_df, test_df))\n",
    "    df = df.sort_values(by=[\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "\n",
    "    # Create column with dates with freq of dataset\n",
    "    len_series = df.groupby(\"unique_id\").agg({\"ds\": \"max\"}).reset_index()\n",
    "    dates = []\n",
    "    for i in range(len(len_series)):\n",
    "        len_serie = len_series.iloc[i, 1]\n",
    "        ranges = pd.date_range(start=\"1970/01/01\", periods=len_serie, freq=freq)\n",
    "        dates += list(ranges)\n",
    "    df.loc[:, \"ds\"] = dates\n",
    "\n",
    "    df = df.merge(m4_info, left_on=[\"unique_id\"], right_on=[\"M4id\"])\n",
    "    df.drop(columns=[\"M4id\"], inplace=True)\n",
    "    df = df.rename(columns={\"category\": \"x\"})\n",
    "\n",
    "    X_train_df = df[df[\"split\"] == \"train\"].filter(items=[\"unique_id\", \"ds\", \"x\"])\n",
    "    y_train_df = df[df[\"split\"] == \"train\"].filter(items=[\"unique_id\", \"ds\", \"y\"])\n",
    "    X_test_df = df[df[\"split\"] == \"test\"].filter(items=[\"unique_id\", \"ds\", \"x\"])\n",
    "    y_test_df = df[df[\"split\"] == \"test\"].filter(items=[\"unique_id\", \"ds\", \"y\"])\n",
    "\n",
    "    X_train_df = X_train_df.reset_index(drop=True)\n",
    "    y_train_df = y_train_df.reset_index(drop=True)\n",
    "    X_test_df = X_test_df.reset_index(drop=True)\n",
    "    y_test_df = y_test_df.reset_index(drop=True)\n",
    "\n",
    "    return X_train_df, y_train_df, X_test_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "\n",
    "def prepare_m4_data(dataset_name, directory, num_obs):\n",
    "    \"\"\"Pipeline that obtains M4 times series, tranforms it and\n",
    "    gets naive2 predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_name: str\n",
    "        Frequency of the data. Example: 'Yearly'.\n",
    "    directory: str\n",
    "        Custom directory where data will be saved.\n",
    "    num_obs: int\n",
    "        Number of time series to return.\n",
    "    \"\"\"\n",
    "    m4info_filename = maybe_download(\"M4-info.csv\", directory)\n",
    "\n",
    "    dailytrain_filename = maybe_download(\"Train/Daily-train.csv\", directory)\n",
    "    hourlytrain_filename = maybe_download(\"Train/Hourly-train.csv\", directory)\n",
    "    monthlytrain_filename = maybe_download(\"Train/Monthly-train.csv\", directory)\n",
    "    quarterlytrain_filename = maybe_download(\"Train/Quarterly-train.csv\", directory)\n",
    "    weeklytrain_filename = maybe_download(\"Train/Weekly-train.csv\", directory)\n",
    "    yearlytrain_filename = maybe_download(\"Train/Yearly-train.csv\", directory)\n",
    "\n",
    "    dailytest_filename = maybe_download(\"Test/Daily-test.csv\", directory)\n",
    "    hourlytest_filename = maybe_download(\"Test/Hourly-test.csv\", directory)\n",
    "    monthlytest_filename = maybe_download(\"Test/Monthly-test.csv\", directory)\n",
    "    quarterlytest_filename = maybe_download(\"Test/Quarterly-test.csv\", directory)\n",
    "    weeklytest_filename = maybe_download(\"Test/Weekly-test.csv\", directory)\n",
    "    yearlytest_filename = maybe_download(\"Test/Yearly-test.csv\", directory)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    X_train_df, y_train_df, X_test_df, y_test_df = m4_parser(\n",
    "        dataset_name, directory, num_obs\n",
    "    )\n",
    "\n",
    "    return X_train_df, y_train_df, X_test_df, y_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
